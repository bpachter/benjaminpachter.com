---
layout: page
title: Skills
permalink: /skillset/
---

## Profile:

- **Data Analytics & Visualization**:
    - **For more detailed demonstrations, check out the [ETL](https://benjaminpachter.com/ETL/) page.**
    - I have advanced experience in **Excel Modeling**, **Power Query**, and **Power BI** for ETL data flows, primarily for creating sophisticated financial models and interactive dashboards that assist in quick and clear strategic decision-making. With proficiency in **SQL**, **DAX**, and **M scripting**, I can move and transform data effectively in a clean and centralized manner that provides maximum transparency and data refresh speed.
    - I am also highly capable within, **Tableau**, **SAS**, **Dynamics**, and **Apache** technologies such as **Spark**, **Airflow**, **Kafka**, **Storm**, **Hadoop**, **Cassandra**, **NiFi**, and **Hive** for big data processing, real-time data streaming, and distributed computing.
    - Additionally, I have operated within many cloud platforms such as **Salesforce**, **Google BigQuery**, **Amazon Redshit**, and **Azure Synapse**. 

- **Programming**:
    - **For more detailed demonstrations, check out the [Algorithms](https://benjaminpachter.com/trade_algos/) page.**
    - I love to code! I find researching new fintech concepts extraordinarily entertaining; I spend lots of time on public academic websites such as Quant Stack Exchange, SSRN, IMF, and Coursera to stay on the frontlines of newly developed ideas and technologies. I have extensive experience pushing complex data combinations into the above visualization tools to tell interesting stories.
    - My favorite and most commonly utilized technologies are **Python**, **R**, **C++**, and **JavaScript**, but I have aggregated my total code/scripting experience below:

        - Python
        - R
        - C++
        - C#
        - JavaScript
        - Java
        - Salesforce Apex
        - SQL
        - Go
        - MQ4/MQ5
        - Scala
        - Pine
        - Rust
        - VBA
        - Ruby
        - PHP
        - Swift
        - MATLAB
        - SAS
        - HTML
        - CSS
        - XML
        - Bash        
        - PowerShell
    

    - And here are some of my favorite **Python libraries**:
        
        - Pandas
        - NumPy
        - SciPy
        - QuantLib
        - Scikit-learn
        - ib_insync
        - XGBoost
        - TensorFlow
        - PyTorch
        - Keras
        - Django
        - PySpark
        - PyArrow
        - PyKafka
        - PyHive
        - Airflow
        - Cassandra
        - Dask
        - fredapi
        - Matplotlib
        - Seaborn
        - Plotly
        - Bokeh
        - Flask
        - Geopandas
        - tqdm
        - statsmodels
        - FastAPI
        - SQLAlchemy
     

## Financial Modeling, Data Transformation, and Dashboard Development
I develop comprehensive financial and logistical models using **Python**, **SQL**, and **Power Query** to extract, transform, and load versatile data structures to tell compelling stories. The neutrality of raw data has always fascinated me, and when harnessed and channeled correctly, unbiased numbers can be translated into unbiased results that drive clear and confident decision-making.

Initializing business and economic narratives through data transformations and coding has become my hobby these days; I am personally motivated to consistently increase my tech capabilities through my own trading and investing decision optimization. Plus, it is clear that the future is filled with robots; leveraging code is the single most important tool we have available to drive into this reality!

```The robot revolution has already arrived - we just keep them in data centers and servers. - Naval Ravikant```

I've mastered traditional reporting and interactive modeling through **Excel** and **Power BI**, though I have been involved in many other technologies. Automation is the name of the game these days, and I don't spend as much time on the front-end of Excel as I used to. I'd rather transform the data in cloud servers and code the steps in a more efficient and centralized **ETL** structure before the data even gets to Excel. However, I still love to work in Excel models whenever necessary!


# Algorithms:
In my previous roles, including my time at NYMT, Deloitte, and BMW, I have designed and implemented a variety of algorithms to tackle complex problems in finance, data analysis, and optimization. Some of the standard algorithms I have worked on include:

- **Regression, Classification, and Clustering** algorithms for asset selection, credit scoring, diligence profiling, logistics pattern identification, customer segmentation, and CRE asset KPI prediction (rent prices, occupancies, cap rates, etc):
  - Everyday libraries: **Pandas**, **NumPy**, **SciPy**, **scikit-learn** for linear/logistic regression, support vector machines, and clustering algorithms, **XGBoost** and **LightGBM** for gradient boosting, and **TensorFlow**/**PyTorch** for complex neural network models and large-scale data clustering, and **Keras** for deep learning models.
  - Additional languages and libraries: **R** (**caret**, **randomForest**), **JavaScript** (**D3.js** for data visualization), **SQL** for database querying, **Go** for high-performance computing tasks, and **PySpark** (**Spark MLlib** and **Spark SQL**) for scaling single machine use to clusters of machines.

- **Optimization** algorithms for portfolio optimization and resource allocation:
  - Everyday libraries: **SciPy** for mathematical optimization, **cvxpy** for convex optimization problems, **PyPortfolioOpt** for asset portfolio optimization, **NumPy** and **Pandas** for data manipulation and numerical operations, and **PuLP** for linear programming. For larger-scale problems, I use **TensorFlow** and **PyTorch** for heavier optimization tasks with big data architecture.
  - Additional languages and libraries: **MATLAB** for high-level mathematical computations, **PySpark** (**Spark MLlib** for scalable optimization algorithms), **R** (**ROI**, **quadprog**), and **Java** (**OptaPlanner** for planning and scheduling).

- **Time series analysis** for economic, financial, sales, and transportation forecasting:
  - Everyday libraries: **Pandas** and **statsmodels** for ARIMA models, and **TensorFlow**/**Keras** for recurrent neural networks (RNNs) and long short-term memory networks (LSTMs).
  - Additional languages and libraries: **prophet** for forecasting, **R** (**forecast**, **tseries**), **PySpark** (using **Spark MLlib** for large-scale time series analysis), and **Apache Kafka** for real-time data streaming into time series analysis pipelines.

- **Ensemble methods** for improved prediction accuracy and robustness:
  - Everyday libraries: **scikit-learn** for random forests and bagging, **XGBoost**/**LightGBM** for gradient boosting, and **CatBoost** for categorical data handling.
  - Additional languages and libraries: **H2O.ai** for scalable ensemble methods, **R** (**randomForest**, **gbm**).

- **Dimensionality reduction** for enhancing model training efficiency and visualization clarity through feature scaling, reducing noise and redundancy:
  - Everyday libraries: **scikit-learn** for PCA and t-SNE, and **UMAP** for advanced dimensionality reduction techniques.
  - Additional languages and libraries: **R** (**Rtsne**, **PCAtools**), **MATLAB** (for built-in dimensionality reduction functions), and **PySpark** (using **Spark MLlib** for scalable PCA).

- **Model Evaluation and Hyperparameter Tuning** to optimize model performance:
  - Everyday libraries: **scikit-learn** for model evaluation metrics (e.g., accuracy, precision, recall, F1 score) and hyperparameter tuning techniques like grid search and random search.
  - Additional languages and libraries: **Hyperopt** for Bayesian optimization in Python, **Optuna** for advanced hyperparameter optimization, **Keras Tuner** for tuning deep learning models, **PySpark** (using **Spark MLlib** for scalable model evaluation and hyperparameter tuning).


# Models:
In past experiences, I have developed and supported many different kinds of models, but the most notable types I find myself in most of the time are as follows:

- **Underwriting models** for sizing up multifamily investment properties and other CRE asset types:
  - Everyday software: **Excel**/**VBA** for financial modeling and scenario analysis, **Power Query** for data extraction and transformation, **QlikView** for data manipulation and visualization, and **Looker** for advanced data visualization and business intelligence.
  - Additional tools: **Python** for data analysis and ETL pipelines, **JavaScript** for interactive web-based data visualization.

- **Logistics models** that transform raw data into identifiable patterns that promote decision-making:
  - Everyday software: **Alteryx** for data preparation, blending, and advanced analytics, **Talend** for robust data integration and ETL processes, **Power BI** for creating interactive and insightful data visualizations, and **SQL** for efficient database querying and management.
  - Additional tools: **Bash** for automating data processing workflows, **Python** for advanced data manipulation and analysis, **JavaScript** for developing interactive and dynamic data dashboards.

- **Portfolio models** for property, bond, equity, and options holdings evaluations:
  - Everyday software: **Excel**/**VBA** for portfolio modeling and analysis, **Power BI**/**Tableau** for advanced data visualization through interactive dashboards, **C++** for high-performance financial calculations within integrated **Bloomberg Terminal** for real-time market data and management.
  - Additional tools: **Python** in place of C++ and SQL depending on cloud data connections.

- **Monte Carlo models** for predicting interest rate risk for securitized portfolios, carry trades, short-volatility trades, etc.:
  - Everyday software: **Python** using **NumPy** and **SciPy** for probabilistic modeling, **R** for advanced statistical computing and graphics, and **QuantLib** for comprehensive financial modeling, including yield curve and options model risk management, value-at-risk (VaR) calculations, sensitivity analysis, and stress testing.
  - Additional tools: **C++** for implementing high-performance Monte Carlo simulations when dealing with big data projects, though I prefer Python here.

- **Black-Scholes pricing models** for determining the fair value of options portfolios and optimizing risk management:
  - Everyday software: **Python** libraries **NumPy**, **SciPy**, and **QuantLib** for advanced options pricing and risk management, **R** for statistical analysis and modeling, and **Bloomberg Terminal** for accessing real-time market data and financial analytics.
  - Extensive expertise:
    - **The Black-Scholes Model**: Deep understanding and application of the Black-Scholes model, including the derivation and implementation of the Black-Scholes partial differential equation (PDE) for American and European-settled options. This involves calculating the theoretical value of options based on factors like the underlying asset price, strike price, time to expiration, risk-free rate, and volatility.
    - **Options Greeks**:
      - **Delta**: Measuring the rate of change of the option price with respect to changes in the underlying asset price. Experience involves implementing delta-hedging strategies to create delta-neutral portfolios, thereby reducing the directional risk associated with market movements. Delta hedging involves adjusting the positions in the underlying asset to maintain a delta-neutral position as the market fluctuates.
      - **Gamma**: Measuring the rate of change of Delta with respect to changes in the underlying asset price. Expertise in managing Gamma environment for portfolios as well as the market-wide environment to understand the curvature risk and its impact on the sensitivity of delta hedges. By default, my models produce gamma-positioning charts to visualize the exposure and implement gamma scalping techniques to adjust delta hedges more efficiently and mitigate the impact of large price movements.
      - **Theta**: Calculating the rate of change of the option price with respect to the passage of time; managing time decay by analyzing the theta values of options portfolios and adjusting strategies to optimize the timing of trades. This includes sell-side strategies to capitalize on time decay and understanding the implications for long-term vs. short-term options strategies.
      - **Vega**: Measuring the sensitivity of the option price to changes in the volatility of the underlying asset. Experience involves developing volatility trading strategies and indicators, such as long volatility positions to profit from increasing volatility or short volatility positions to benefit from decreasing volatility. This involves constructing and managing positions to exploit volatility arbitrage opportunities.
      - **Rho**: Understanding the sensitivity of the option price to changes in the risk-free interest rate. Experience in incorporating Rho into interest rate risk management strategies, including adjusting the composition of options portfolios in response to anticipated changes in interest rates, and using interest rate derivatives to hedge against fluctuations in the risk-free rate.
      - **Charm**: Measuring the rate of change of Delta-decay over time with respect to the underlying asset's price. This involves monitoring the impact of Charm on options portfolios, especially for near-the-money short-dated options where Delta changes rapidly, which usually leads to adjusting hedging strategies to account for the rapid time-decay effect on Delta and ensuring that the portfolio remains balanced as expiration approaches.
    - **General Concepts**:
      - **Implied Volatility (IV)**: Deep knowledge of implied volatility and its role in options pricing. Experience in constructing and interpreting volatility surfaces, which involves analyzing level-3 market data to understand the implied volatility for different strikes and maturities, and using this information to inform trading and risk management decisions.
      - **Volatility Smile/Surf**: Understanding the phenomenon of volatility smiles and skews, and how volatility momentum affects the pricing and risk management of options portfolios. This includes analyzing the IV pattern across different strikes and expirations to detect anomalies that produce arbitrage opportunities.
      - **Risk-neutral Valuation**: Applying the principles of risk-neutral valuation to ensure accurate and consistent pricing of options. This involves using risk-neutral probabilities to discount expected future payoffs and calculate the fair value of options under different market conditions.
    - **Risk Management Strategies**:
      - Extensive experience in using the Black-Scholes model for managing the risks associated with options portfolios. This includes implementing delta hedging to neutralize price risk, gamma scalping to manage curvature risk, and vega risk management to control volatility exposure.
      - Proficient in developing and implementing strategies to mitigate the impact of adverse market movements on options positions. This involves continuously monitoring the options portfolio and making adjustments to hedges and positions to maintain a balanced risk profile.
    - **Implementation**:
        - **Custom Black-Scholes Pricing Tools**: 
            - Developed using **Python** and **C++** for real-time calculations and integration with trading platforms.
            - **Python** for rapid prototyping and backtesting with libraries like **QuantLib**, **NumPy**, **SciPy**, and **Pandas**.
            - **C++** for high performance and low latency in trading applications.

        - **QuantLib Integration**:
            - Utilized for rapid and easy financial modeling and custom financial instruments.
            - Since this is a Python library, options-pricing models can be easily integrated through QuantLib anywhere you can push a Python script for flexible, scalable custom proprietary models.

        - **Data Integration and Real-time Analytics**:
            - Integrated market data from **Bloomberg Terminal** via **C++**, **JavaScript**, and **Python** for real-time prices, volatility surfaces, and interest rates.
            - Implemented data pipelines with **Python** and ETL tools like **SQL**/**Power Query** and **Apache** Spark technology for clean, up-to-date data.

        - **Backtesting, Validation, and Documentation**:
            - Conducted backtesting using historical data and cross-referenced data to validate models and trading strategies.
            - Collaborated with analysts and traders to refine models based on backtesting results.
            - Created comprehensive documentation for tools and models, including user guides and technical specs.
            - Provided training and support to traders and risk managers, including workshops and tutorials.

# Blockchain & Distributed Systems
I have hands-on experience with **blockchain** technologies and **distributed systems**, developing proficiency along the way in **Rust**, **Solidity**, and **Hyperledger Fabric** to develop secure and efficient decentralized applications.

- **Blockchain Development:**
    - **Rust**: Utilized Rust for developing high-performance, memory-safe applications in blockchain projects, including building components for decentralized systems and optimizing blockchain protocols.
    - **Solidity**: Developed smart contracts on the Ethereum blockchain using Solidity, implementing decentralized finance (DeFi) protocols and creating tokenized assets.
    - **Hyperledger Fabric**: Deployed and managed enterprise-grade blockchain networks using Hyperledger Fabric, focusing on permissioned blockchains for secure and private business transactions.
- **Distributed Systems:**
    - Designed and implemented distributed systems for real-time data processing and analytics using **Apache Kafka** and **Apache Spark**.
    - Developed microservices architecture for scalable applications using **Docker** and **Kubernetes** for container orchestration.
- **Smart Contract Development:**
    - Proficient in using **Truffle** and **Hardhat** frameworks for testing, deploying, and managing **Ethereum smart contracts**.
    - Implemented secure smart contracts with libraries like **OpenZeppelin** to ensure best practices in smart contract security and functionality.
- **Decentralized Applications (dApps):**
    - Built interactive dApps using **React** and **ethers.js** for seamless integration with **Ethereum** blockchain.
    - Leveraged IPFS for decentralized storage solutions in blockchain applications.

Outside of my professional career,  I am most interested in backtesting complex options trading strategies and automating these through brokerage APIs, as well as building my own applications. My favorite is retail brokerage automation IBKR, as it is easy to set up a paper trading account that can be directly connected to my TradingView Pine scripts for easy and transparent backtesting.
<br><br>
## Database and Cloud Server Management
In previous roles, I have managed database and cloud infrastructure, utilizing the following technologies:

- **SQL Server Management Studio (SSMS)**:
  - Testing, deploying, and optimizing new stored procedures, views, and tables.
  - Utilizing **T-SQL** for complex queries and procedural extensions, **SQL** for standardized queries, and **MySQL** for relational database management.
  - Implementing database indexing and performance tuning to ensure efficient query execution.
  - Conducting regular database backups and recovery operations.
  - Managing both **SQL** and **NoSQL** databases for varied use cases, including **MongoDB** and **Cassandra** for NoSQL solutions.
  - Comparison of Database Technologies:<br>

| Feature           | SQL                                    | MySQL                                  | T-SQL                                      | NoSQL                                      |
|-------------------|----------------------------------------|----------------------------------------|--------------------------------------------|--------------------------------------------|
| **Schema**        | Predefined schema, structured          | Predefined schema, structured          | Predefined schema, structured              | Schema-less, flexible                      |
| **Scalability**   | Vertical (scale-up)                    | Vertical and Horizontal (scale-up/out) | Vertical (scale-up)                        | Horizontal (scale-out)                     |
| **Consistency**   | Strong consistency (ACID)              | Strong consistency (ACID)              | Strong consistency (ACID)                  | Eventual consistency (some provide strong) |
| **Query Language**| Standardized SQL                       | SQL with MySQL extensions              | SQL with procedural extensions             | Varies by implementation                   |
| **Data Models**   | Relational (tables)                    | Relational (tables)                    | Relational (tables)                        | Document, Key-Value, Column-Family, Graph  |
| **Transactions**  | ACID transactions                      | ACID transactions                      | ACID transactions with procedural extensions| Limited support (varies by implementation) |
| **Use Cases**     | Complex queries, transactions, structured data | Web applications, read-heavy workloads, LAMP stack | Complex business logic, stored procedures  | Big data, real-time web apps, flexible schemas |



- **Salesforce Development Tools**:
  - **SOQL** for querying Salesforce data, **Apex** for developing custom business logic, and **Flow Automations** for streamlining business processes within CRM sites.
  - **Salesforce DX** for version control and continuous integration.
  - Customizing Salesforce objects and fields to meet business requirements.
  - Creating and managing Lightning components for enhanced user interfaces.

- **Cloud Platforms**:
  - **Azure**: Implementing scalable cloud solutions, managing **Azure SQL Database**, **Azure Cosmos DB**, and using **Azure Data Factory** for ETL processes.
  - **Google Cloud Platform (GCP)**: Utilizing **BigQuery** for large-scale data analysis, **Cloud Storage** for secure and scalable storage solutions, and **Dataflow** for real-time data processing.
  - **Amazon Web Services (AWS)**: Managing **RDS** for relational database services, **DynamoDB** for NoSQL database needs, and using **Lambda** for serverless computing solutions.

- **Real-time Data Management**
    - Managed real-time data streams and processing using **Apache Kafka**, **Spark Streaming**, **Apache Flink**, and **AWS Kinesis** for high-throughput, low-latency data handling and analytics.
    - Ensured real-time read/write access and integration with distributed storage systems using **Apache HBase** with **Hadoop**, and leveraged **Google Cloud Pub/Sub** for event-driven messaging and scalable data pipelines with **Dataflow**.

<br>

## Closing Thoughts
Thank you for taking the time to review my professional skill set and experiences! I am passionate about leveraging data analytics, programming, and cutting-edge technologies to drive strategic decision-making and optimize business processes. I am eager to apply my diverse background in financial modeling, data augmentation, algorithm development, and blockchain technologies while continuing to grow in a dynamic and challenging environment. If you have any questions or would like to discuss potential collaborations, please feel free to reach out. I look forward to the opportunity to work together and achieve great things together!!

Cheers,  
Benjamin Pachter